{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "-------------------\n",
    "# Tarea Numerica N°2, Calculo en Varias Variables  (MA-2001)\n",
    "-------------------\n",
    "> Integrantes: \\\n",
    ">$\\rightarrow$ Jorge Cummins;  RUT: 21.353.175-1 \\\n",
    ">Seccion: 5 \\\n",
    ">Profesor de Catedra: Claudio Muñoz. \\\n",
    ">Fecha de Entrega: 28 de Mayo de 2023\n"
   ],
   "metadata": {
    "id": "ZIsH_HPjvhdS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Sobre el Documento"
   ],
   "metadata": {
    "id": "2Ua7LZykxSuR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Indice:\n",
    ">   1.  ...................................................... (Parte 1.) Descripcion Teorica\n",
    ">   2.  ...................................................... (Parte 2.) \"Numerico\" [(a) y (b)]\n",
    ">   3.  ...................................................... Palabras Finales\n",
    ">\n",
    "> ### Componentes:\n",
    "> El presente documento es el solicitado en formato \"*jupyter Notebook*\" de la tarea numerica solicitada. En esta se encontraran cuadros de codigo, algunas anotaciones, cuadros de texto explicativos además de algunos ejemplos si la situacion lo amerita. Esto con el fin de servir como argumentacion o solucion directa a la evaluacion.\n",
    ">\n",
    "> Los cuadros de texto corresponeran a la información sobre el codigo que le prosigue siguiendo un formato de \"*texto arriba, codigo abajo*\", el cual intentara cumplir el objetivo de explicar el funcionamiento del codigo en el presente. Pese a esto también existiran anotaciones extras en formato \"# *Anotaciones*\" color \"verde\" en el/los codigos que deberia tener un formato similar al siguiente:\n",
    "```\n",
    "1 # Anotacion\n",
    "2 Este es un codigo:              # Anotacion\n",
    "3     sirve para ejemplificar     # Anotacion\n",
    "4     el formato que se dijo      # Anotacion \n",
    "5                                 #\n",
    "6 previamente                     # Anotacion\n",
    "```\n",
    "> Estos seran parte del codigo pero no tendran funcion ninguna más que ser anotaciones insertadas por los desarrolladores para uso personal, aunque de ser necesario puede utilizarse como información sobre el documento/codigo.\n",
    "> ### Orden:\n",
    "> El documento estara estructurado mediante diferentes partes que se enumeran en el \"Indice\", en la primera parte (Parte 1.) y la segunda (Parte 2.) se encuentran las soluciones respectivas, mientra que en \"Palabras Finales\", comentarios respectivos a la realizacion del documeno o tarea.\n",
    ">\n",
    "> Es importante informar que de ser necesario, se utilizaran codigos compartidos con el fin de mantener el orden y optimizar el proceso, el cual será denotado por una anotacion al inicio del cuadro de codigo de forma \"codigo compartido\". Para poder utilizar el codigo donde esté siendo reutilizado, será necesario inicializar el original. Un caso será la siguiente línea de codigos que permitira el acceso a librerias que podrian ser utilizadas.\n"
   ],
   "metadata": {
    "id": "9GiLnnngJaEl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Parte 1. Descripcion Teorica"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tNuta-PFSkTC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Dado a las caracteristicas de la entrega, en la cual ademas del documento prescente, se entrego un informe en formato PDF donde se contendran las soluciones respectivas.\n",
    "> Sin embargo, debido a que la realizacion de la tarea depende de los resultados obtenidos en esta parte, se agregan los siguientes resultados obtenidos de manera resumida:\n",
    "> #### (a) Matriz Jacobiana\n",
    "> \\begin{align*} J_{\\mathcal{R}(\\Phi)(x_0)} &= \\begin{pmatrix} \\sigma'(w_{1}x_{0,1}+w_{2}x_{0,2}+b)w_{1} & \\sigma'(w_{1}x_{0,1}+w_{2}x_{0,2}+b)w_{2}\\end{pmatrix} \\\\ \\iff J_{\\mathcal{R}(\\Phi)(x_0)} &= \\begin{pmatrix}\\frac{w_{1}e^{(w_{1}x_{0,1} + w_{2}x_{0,2} + b)}}{(1+e^{(w_{1}x_{0,1} + w_{2}x_{0,2} + b)})^2}  & \\frac{w_{2}e^{(w_{1}x_{0,1} + w_{2}x_{0,2} + b)}}{(1+e^{(w_{1}x_{0,1} + w_{2}x_{0,2} + b)})^2}\\end{pmatrix}\\end{align*} \\\n",
    "> #### (b) Calculo Derivadas Parciales Funcion de Costos\n",
    "> \\begin{align*} \\frac{\\partial C}{\\partial w_{1}} &= \\sum_{i=1}^{N}(\\mathcal{R}(\\Phi)\\hat{x}_{0}^{i}-\\hat{x}_{f}^{i})\\sigma'(w_{1}x_{0,1}^{i}+w_{2}x_{0,2}^{i}+b){x}_{0,1}^{i} \\\\ \\frac{\\partial C}{\\partial w_{2}} &= \\sum_{i=1}^{N}(\\mathcal{R}(\\Phi)\\hat{x}_{0}^{i}-\\hat{x}_{f}^{i})\\sigma'(w_{1}x_{0,1}^{i}+w_{2}x_{0,2}^{i}+b){x}_{0,2}^{i}\\\\ \\frac{\\partial C}{\\partial b} &= \\sum_{i=1}^{N}(\\mathcal{R}(\\Phi)\\hat{x}_{0}^{i}-\\hat{x}_{f}^{i})\\sigma'(w_{1}x_{0,1}^{i}+w_{2}x_{0,2}^{i}+b)\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "id": "rX0i-LEuSkTD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Parte 2. \"Numerico\""
   ],
   "metadata": {
    "collapsed": false,
    "id": "i3Xdqgi7SkTD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> La parte 2 de la tarea solicitada, se baso en dos diferentes estudios (aunque relacionados en la tematica presente) a programar. Esto se dividion en dos etapas para realizar la respectiva funcion.\n",
    "> Sin embargo, estas dos etapas llegaron a tener documentos o funcionalidades en comun, para lo cual se agregaron los siguientes codigos y celdas de codigos para optimizar el orden y proceso con estas funciones."
   ],
   "metadata": {
    "collapsed": false,
    "id": "nZZRfcF3SkTE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "########################## Librerias Importadas ################################\n",
    "# Codigo compartido |--> Librerias export importadas\n",
    "import numpy as np\n",
    "########################## Librerias Importadas ################################\n",
    "# =============================================================================="
   ],
   "metadata": {
    "id": "ud8Ehlcbzv3D",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.013109200Z",
     "start_time": "2023-05-28T21:10:47.953826500Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "########################### Valores Utilizados# ################################\n",
    "# Lista de tuplas o instancias.\n",
    "D = [(9.0,7.0,0.0), (2.0,5.0,1.0), (3.2,4.94,1.0), (9.1,7.46,0.0),\n",
    "     (1.6,4.83,1.0), (8.4,7.46,0.0), (8.0,7.28,0.0), (3.1,4.58,1.0),\n",
    "     (6.3,9.14,0.0), (3.4,5.36,1.0)]\n",
    "\n",
    "\n",
    "# Vector o punto de realizacion\n",
    "x0 = (8, 7)                                                                      # Tupla que representa al vector\n",
    "\n",
    "\n",
    "# Condiciones iniciales/constantes\n",
    "k = 1000                                                                         # Cantidad de iteraciones\n",
    "l = 0.01                                                                         # Learning rate de la red neuronal\n",
    "########################### Valores Utilizados# ################################\n",
    "# =============================================================================="
   ],
   "metadata": {
    "id": "kp3lRUeASkTG",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.016050900Z",
     "start_time": "2023-05-28T21:10:47.961319100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### (a) Programar algoritmo que \"Entrenara\" redes neuronales\n",
    "> Durante la etapa \"$a)$\", se solicito la realizacion del algoritmo llamado \"gradiente conjugado\" para los diferentes \"$k$\" dado a $k = 1,...,M$.\n",
    "> El algoritmo tiene la principal funcion de realizar la siguiente iteracion:\n",
    ">\\begin{align*} w_{1}^{k+1} &= w_{1}^{k}-l\\frac{\\partial C}{\\partial w_{1}} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\\\ w_{2}^{k+1} &= w_{2}^{k}-l\\frac{\\partial C}{\\partial w_{2}} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\\\ b^{k+1} &= b^{k}-l\\frac{\\partial C}{\\partial b} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\end{align*}\n",
    "> Con $(w_{1}^{0}, w_{2}^{0}, b^{0}) \\in \\mathbb{R}^{3}$ como inicio, dados y aleatorios.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZavpqEIJSkTG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Considerando el algoritmo propuesto, el codigo (o los respectivos cuadros de codigos) fueron basados en torno a las diferentes funciones o calculos realizados en la parte teorica. Especificamente para el caso de las derivadas parciales y las realizaciones. De esta manera, se llego a tres principales grupos de funciones que estaban prescentes durante el proceso: \\\n",
    ">\n",
    "> \\begin{enumerate} 1. & \\text{Realizacion Red Neuronal} \\\\ 2. & \\text{Derivadas Parciales de la Funcion de Costo} \\\\ 3. & \\text{Gradiente Conjugado} \\end{enumerate} \\\n",
    "> Estos grupos fueron ordenados en respectivos cuadros de codigos de la manera siguiente:\n",
    "> #### 1. Realizacion Red Neuronal\n",
    "> En este caso, se encontrarian las funciones que realizan el proceso de obtencion de una red neuronal de manera estandar siempre y cuando cumpla las mismas condiciones a las que fueron impuestas inicialmente. \\\n",
    "> De las funciones realizadas, se tienen las siguientes: \\\n",
    ">\n",
    ">  $\\rightarrow$ \"sigmoid\": Tiene la importancia de ser la funcion de activacion sigma ($\\sigma$).\n",
    "> Teoricamente es de la forma:\n",
    "> \\begin{align*} \\sigma(s) &= \\frac{e^{s}}{(1+e^{s})} \\\\ \\sigma'(s) &= \\frac{e^{s}}{(1+e^{s})^{2}} \\end{align*}\n",
    "> Donde $\\sigma(s)$ corresponde a la funcion en especificio y, $\\sigma'(s)$ a su derivada respectivamente. Esta, a la hora de programarse, se realizo de manera tal que se pudiera seleccionar la forma derivada como no derivada (dependiendo del parametro seleccionado).\\\n",
    ">\n",
    ">  $\\rightarrow$ \"f\": Analogo a los resultados obtenidos en el ambito teorico, corresponde a los puntos evaluador por la funcion de activacion, asi permitiendo reescribir la realizacion $\\mathcal{R}(\\Phi)$.\n",
    "> Teoricamente serian de la forma: \\\n",
    "> \\begin{align*} f(x) &= w_{1}x_{0,1}+w_{2}x_{0,2}+b \\\\ \\implies \\mathcal{R}(\\Phi)(x_{0}) &= \\sigma(w_{1}x_{0,1}+w_{2}x_{0,2}+b) \\\\ \\iff \\mathcal{R}(\\Phi)(x_{0}) &= \\sigma(f(x_{0})) & (\\sigma o f)(x_{0})\\  (\\text{analogo})\\end{align*}\\\n",
    ">\n",
    "> $\\rightarrow$ \"Realizacion\": Se encarga de realizar la red neuronal especifica para esste caso. Esta, a diferencia de lo obtenido en indoles similares, > > sera especifica para este caso en que, se estimo una realizacion o funcion para realizacion con forma constante durante todo el desarrollo. En el ambito teorico, la funcion se veria de la siguiente forma: \\\n",
    "> \\begin{align*} Realizacion &:= \\mathcal{R}(\\Phi)(x_{0}), & (\\text{Con}\\ x_{0}\\in \\mathbb{R}^{2}) \\\\ \\implies Realizacion &= \\sigma(f(x_{0})) = \\sigma(w_{1}x_{0,1}+w_{2}x_{0,2}+b) \\end{align*}\n",
    "> Si bien la explicacion de estas funciones puede ser redundante en el ambito teorico, no lo es en el sentido de programar estas ultimas, la cual concluyo en que se programaran cada una por separado siguiendo el orden previamente explicado. Finalmente, el codigo encargado de realizar esta tarea seria el siguiente:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================\n",
    "############################## Realizacion Red #################################\n",
    "# Funcion sigmoid\n",
    "# Toma dos numeros y devuelve la funcion sigmoid o su derivada\n",
    "#           => 0, devuelve la funcion\n",
    "#           => 1, devuelve la derivada en torno a s\n",
    "def sigmoid(s,tipo):\n",
    "    e = np.exp(s)                                                                # Evalua la exponencial en el punto \"s\"\n",
    "    denominador = (1+e)                                                          # Denominador de la funcion\n",
    "    if tipo == 0:                                                                # Condicional para evaluar funcion\n",
    "        sigmoide = (e)/(denominador)\n",
    "        return sigmoide\n",
    "    elif tipo == 1:                                                              # Condicional para evaluar derivada\n",
    "        sigmoide = (e)/((denominador)**2)\n",
    "        return sigmoide\n",
    "\n",
    "\n",
    "# Funcion f(X)\n",
    "# Toma una tupla de vectores (x1,x2) y una tupla phi (w1, w2, b) y,\n",
    "# devuelve la realizacion no evaluada de la red neuronal \n",
    "def f(phi,x):\n",
    "    x1,x2 = x                                                                    # Obtiene valores Tupla\n",
    "    w1,w2,b = phi                                                                # Obtiene valores Phi\n",
    "    fx = w1*x1+w2*x2+b                                                           # Calcula pre-realizacion\n",
    "    return fx\n",
    "\n",
    "# Funcion Realizacion de la red neuronal (Rphi(X0))\n",
    "# Toma la tupla de valores y el vector Phi para devolver\n",
    "# la realizacion de la red neuronal.\n",
    "# Teoricamente corresponde al calculo de:\n",
    "# R(Phi)(X0) = sigma(f(x))\n",
    "def realizacion(phi,x,dif=0): \n",
    "    fx = f(phi,x)                                                                # obtencion f(x)\n",
    "    gof = sigmoid(fx,dif)                                                        # composicion\n",
    "    return gof\n",
    "############################## Realizacion Red #################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "Jy1pGjynSkTH",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.017048300Z",
     "start_time": "2023-05-28T21:10:47.972179Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### 2. Derivadas Parciales de la funcion de costos\n",
    "> Para el caso de las derivadas parciales, se construyo un cuadro de codigos inspirado en el formato de la realizacion de la red neurona para las diferentes funciones presentes. Sin embargo, en vez de realizar diferentes tareas, estas se encontrarian en un desarrollo que se describiria cualitativamente como \"ciclico\", pues de las tres funciones generadas, una se encargaria del calculo de las derivadas parciales, otra para calcular las derivadas parciales de una coleccion de datos ingresadas y, la ultima, del calculo de las derivadas parciales de la funcion de costos. Estas, llevadas a un ambito teorico seguirian el siguiente orden: \\\n",
    ">\n",
    "> $\\rightarrow$ \"parcial\": Funcion encargada del calculo de una derivada parcial\n",
    ">\n",
    "> $\\rightarrow$ \"derivadaParcial\": Funcion encargada del calculo de las derivadas parciales para cada componente presente en una coleccion de datos. Llevandolo al ambito teorico, se podria basar en la siguiente ecuacion: \\\n",
    "> \\begin{align*} D &= \\left\\{\\hat{x_{0}}^{i}, \\hat{x_{f}}^{i}\\right\\}_{i=1,\\_,N} \\\\ \\implies derivadaParcial &:= \\left\\{(\\frac{\\partial C}{\\partial w_{1}},\\frac{\\partial C}{\\partial w_{2}}, \\frac{\\partial C}{\\partial b})^{i}\\right\\}_{i=1,\\_,N} \\end{align*}\n",
    "> Una de las principales ideas, es poder generar el conjunto de derivadas parciales a utilizar.\n",
    ">\n",
    "> $\\rightarrow$ \"dcosto\": Relacionada con la funcion anterior, se encarga de sumar cada derivada parcial de cuyas variables en las cuales se esta derivando sean iguales. En el ambito teorico tendria la siguiente forma: \\\n",
    "> \\begin{align*}  D &= \\left\\{\\hat{x_{0}}^{i}, \\hat{x_{f}}^{i}\\right\\}_{i=1,\\_,N} & (\\text{Coleccion de datos utilizadas}) \\\\ dcosto &:= [{(\\sum_{i=1}^{N} (\\frac{\\partial C}{\\partial w_{1}})_{i}, \\sum_{i=1}^{N} (\\frac{\\partial C}{\\partial w_{2}})_{i}, \\sum_{i=1}^{N} (\\frac{\\partial C}{\\partial b})_{i})}] & \\end{align*}\n",
    "> En la cual, \"$N$\" Corresponde a la cantidad de datos que hay en la coleccion \"$D$\". \\\n",
    ">\n",
    "> Si bien los resultados parecieran ser similares, para resumir lo explicado, la primera funcion se encargaria de realizar las calcular las derivadas parciales, mientras que la segunda, la suma de estas. Finalmente, el codigo quedaria de la forma:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================\n",
    "############################ Derivadas Costo ###################################\n",
    "# Funcion Parcial\n",
    "# Realiza el calculo de una derivada parcial\n",
    "def Parcial(phi,datox):\n",
    "    x1d, x2d, xfd = datox\n",
    "    dato = x1d, x2d\n",
    "    fx = f(phi,dato)\n",
    "    C = sigmoid(fx,0)-xfd\n",
    "    dC = sigmoid(fx,1)\n",
    "    dw1 = C*dC*x1d\n",
    "    dw2 = C*dC*x2d\n",
    "    db = C*dC\n",
    "    gradx = (dw1, dw2, db)\n",
    "    return gradx\n",
    "# Funcion derivada parcial\n",
    "# Realiza el calculo de todas las derivadas parciales en una coleccion de datos\n",
    "def derivadaParcial(phi,datos,gradn):\n",
    "    N = len(datos)+1\n",
    "    if N==1:\n",
    "        return gradn\n",
    "    else:\n",
    "        datox = datos[1:N]\n",
    "        return derivadaParcial(phi, datox, gradn=gradn+ [Parcial(phi, datos[0])])\n",
    "\n",
    "\n",
    "# Funcion dcostos\n",
    "# Realiza la derivada de la funcion de costos, para lo cual en base a las funciones\n",
    "# previas, suma las derivadas respectivas a cada componente\n",
    "def dcosto(phi,datas):\n",
    "    datos = derivadaParcial(phi,datas,gradn=[])\n",
    "    dw1 = []\n",
    "    dw2 = []\n",
    "    db = []\n",
    "    N=len(datos)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        w1 = datos[i][0]\n",
    "        w2 = datos[i][1]\n",
    "        b = datos[i][2]\n",
    "        ddw1 = dw1 + [w1]\n",
    "        dw1 = ddw1\n",
    "        ddw2 = dw2 + [w2]\n",
    "        dw2 = ddw2\n",
    "        ddb = db + [b]\n",
    "        db = ddb\n",
    "        dcostox = [sum(dw1),sum(dw2),sum(db)]\n",
    "        i = i+1\n",
    "    dcosto = dcostox\n",
    "    return dcosto\n",
    "############################ Derivadas Costo ###################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "kJ33dmDbSkTH",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.038018900Z",
     "start_time": "2023-05-28T21:10:47.984127800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### 3. Gradiente Conjugado\n",
    "> Con las funciones previas calculadas, se construyo un algoritmo (en forma de funcion) que permitiera iterar siguiendo la forma del algoritmo propuesto en la tarea. Para este ultimo, se agrego un generador de vectores (o tuplas) $(w_{1},w_{2},b)$ aleatorio con valores entre $[-1, 1]$. La funcion finalmente recibiria la coleccion de datos, una cantidad \"k\" de iteraciones a gusto y una tasa \"l\" de aprendizaje o, \"learning rate\". Finalmente, el codigo quedaria de la forma:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================\n",
    "########################## Gradiente Conjugado #################################\n",
    "# Funcion gradiente Conjugado\n",
    "# Toma una cantidad \"k\" de iteraciones, una tasa \"l\" de aprendizaje y una \n",
    "# coleccion de datos para obtener una aproximacion de la realizacion de una \n",
    "# red neuronal desconocida. Esto en base a un phi aleatorio otorgado.\n",
    "def gradienteConjugado(k,l,datos):  \n",
    "    w1 = np.random.uniform(-1,1)                                                 # -|                  \n",
    "    w2 = np.random.uniform(-1,1)                                                 #  |--->|Definicion de valores w1, w2, b aleatorios entre [-1,1] \n",
    "    b = np.random.uniform(-1,1)                                                  # -|                                       \n",
    "    def gradienteNC(w1,w2,b,k,l,datos):                                          # Se define una funcion que realizara los calculos\n",
    "        phi = w1,w2,b                                                            # Definir una realizacion (Phi) base\n",
    "        if k == 0:                                                               # Caso base en recursividad.\n",
    "            return w1,w2,b                                                     \n",
    "        else:                                                                   \n",
    "            w11 = w1-(l)*float((dcosto(phi,datos)[0]))                           # -|    |Iteracion dada que el gradiente debe cumplir.\n",
    "            w22 = w2-(l)*float((dcosto(phi,datos)[1]))                           #  |--->|Esta obtiene el caso para k+1 iteraciones dado k.\n",
    "            bb = b-(l)*float((dcosto(phi,datos)[2]))                             # -|    |Se realiza para las componentes w1, w2 y b del Phi.\n",
    "            w1 = w11                                                           \n",
    "            w2 = w22                                                            \n",
    "            b = bb                                                            \n",
    "            return gradienteNC(w1,w2,b,k-1,l,datos)                              # Realiza iteracion.\n",
    "    Cgradiente = gradienteNC(w1,w2,b,k,l,datos)                                  # Se llama a la funcion para realizar la operacion.\n",
    "    return Cgradiente\n",
    "########################## Gradiente Conjugado #################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "xXQ4wepISkTI",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.038018900Z",
     "start_time": "2023-05-28T21:10:47.990402500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### (b) Realizar M = 1000 iteraciones del algoritmo con el fin de entrentar la red $\\Phi$. Ademas, encontrar el valor de la realizacion en el punto $(8, 7)$\n",
    "> Dado $D$ con 10 instancias consistentes de dos valores input y un tercero de output en {$0,1$} de la siguiente manera: \\\n",
    "> $D=${{$9, 7.0, 0$}, {$2, 5.0, 1$}, {$3.2, 4.94, 1$}, {$9.1, 7.46, 0$}, {$1.6, 4.83, 1$}, {$8.4, 7.46, 0$}, {$8, 7.28, 0$}, {$3.1, 4.58, 1$}, {$6.3, 9.14, 0$}, {$3.4, 5.36, 1$}} \\\n",
    "> Realizar M=1000 iteraciones del gradiente conjugado (anterior) para lograr el objetivo.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "M5opf7leSkTI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Con el algoritmo (en una funcion) para obtener el gradiente ya realizado, se definio una coleccion de datos como la dada por enunciado en la cual se definio un algoritmo para la obtencion de resultados. En este caso se separo en 3 secciones: \\\n",
    "> \\begin{enumerate} & \\text{I}.\\ \\text{Ejecutar el gradiente en torno a los valores conocidos} \\\\ & \\text{II}.\\ \\text{Evaluar la realizacion dado al gradiente calculado}\\\\ & \\text{III}.\\ \\text{Iterar una cantidad n de veces con el fin de tener un margen de comparacion de resultados} \\end{enumerate}\n",
    "> Para este ultimo punto \"$\\text{III}$\", se entrego desactivado para evitar confusion con resultados, el cual puede ser activado y modificado a gusto retirando los \"#\" de la linea 26 hasta 34 del cuadro de codigo realizado."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS\n",
      "\n",
      "\n",
      "\n",
      "Para la iteracion numero 1000 ,el vector Phi deberia estar dado por: (-1.4384300823316256, 0.9395382154080627, 0.6047745126710885)\n",
      "A su vez, la realizacion estaria dada por: 0.013049132035979441\n",
      "\n",
      "\n",
      "En 5 Tuplas (phi) aleatorias, se tiene el siguiente comportamiento:\n",
      "\n",
      "1 | Gradiente Conjugado (phi): (-1.2782863967654494, 0.6726871098808125, 1.6324846183727366)\n",
      "     Realizacion: 0.02013417978237825\n",
      "\n",
      "2 | Gradiente Conjugado (phi): (-1.2627946551084737, 0.6588495303341282, 1.655387700572937)\n",
      "     Realizacion: 0.021144114611376916\n",
      "\n",
      "3 | Gradiente Conjugado (phi): (-1.3657504409181598, 0.8208788094102979, 1.0488544343320847)\n",
      "     Realizacion: 0.01581278376929511\n",
      "\n",
      "4 | Gradiente Conjugado (phi): (-1.4000009992030824, 0.8335514740517916, 1.113676732426063)\n",
      "     Realizacion: 0.014043245755023274\n",
      "\n",
      "5 | Gradiente Conjugado (phi): (-1.2736025788177072, 0.6408980743162438, 1.8215696693808714)\n",
      "     Realizacion: 0.020213887225868463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================= Tarea 2, Parte 2.b =============================    ______________________\n",
    "########################## Gradiente Conjugado #################################    | Anotaciones Extras |\n",
    "# Desarrollo final                                                                  |____________________|\n",
    "#   I. Ejecutar el gradiente para obtener el Phi = (wk1, wk2, bk2)\n",
    "#   II. Llamando \"phi = gradiente conjugado\", ejercutamos la\n",
    "#       realizacion en (8, 7) y phi\n",
    "#   III. Printeamos n resultados con la cantidad \"n\", numero natural a gusto\n",
    "print('RESULTADOS')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "# I.\n",
    "# Aplicacion del gradiente conjugado\n",
    "phiconjugado = gradienteConjugado(k,l,D)\n",
    "# Mostrar resultados\n",
    "print('Para la iteracion numero', k,',el vector Phi deberia estar dado por:'\n",
    ",phiconjugado)\n",
    "# II.\n",
    "# Obtencion de red neuronal\n",
    "redrealizada = realizacion(phiconjugado,x0,0)\n",
    "# Mostrar resultados\n",
    "print('A su vez, la realizacion estaria dada por:',redrealizada)\n",
    "print()\n",
    "print()\n",
    "# III.\n",
    "n = 5\n",
    "print('En', n,'Tuplas (phi) aleatorias, se tiene el siguiente comportamiento:')\n",
    "print()\n",
    "for i in range(n):\n",
    "    grad = gradienteConjugado(k,l,D)\n",
    "    print(i+1,'| Gradiente Conjugado (phi):', grad)\n",
    "    real = realizacion(grad,x0,0)\n",
    "    print('     Realizacion:', real)\n",
    "    print()\n",
    "########################## Gradiente Conjugado #################################\n",
    "# ============================= Tarea 2, Parte 2.b ============================="
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFcg18SzSkTI",
    "outputId": "08274656-62e8-4ecd-a1f1-a59d66a7a899",
    "ExecuteTime": {
     "end_time": "2023-05-28T21:10:48.941171300Z",
     "start_time": "2023-05-28T21:10:48.008074100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Palabras Finales:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "CbTNIbOwSkTJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Para ir finalizando el desarrollo numerico, la realizacion de esta ultima permite tener una idea del funcionamiento y de los resultados que las herramientas utilizadas tienen en un contexto de aplicacion como podria ser el encontrar una realizacion de una red neuronal. En esta ultima, mediante la programacion de los algoritmos solicitados utilizando los mecanismos del calculo multivariable, especificamente en la busqueda de optimizar (entiendase por optimizar, maximizar o minimizar resultados de algun proceso) un proceso como fue este en concreto, en donde con el fin de obtener un aproximacion de la realizacion de una red neuronal, se minimizo el error asociado al costo que esta ultima tiene. \\\n",
    "> Es importante reiterar que, los calculos o, de manera general, el resto de la realizacion de la tarea se encuentran en un informe externo al \"jupyter notebook\" prescente, pero el cual se decidio agregar informacion con el fin de preservar la formalidad y orden del codigo realizado.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8ohZdTxuSkTK"
   }
  }
 ]
}
