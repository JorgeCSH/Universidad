{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "-------------------\n",
    "# Tarea Numerica N°2, Calculo en Varias Variables  (MA-2001)\n",
    "-------------------\n",
    "> Integrantes: \\\n",
    ">$\\rightarrow$ Jorge Cummins;  RUT: 21.353.175-1 \\\n",
    ">Seccion: 5 \\\n",
    ">Profesor de Catedra: Claudio Muñoz. \\\n",
    ">Fecha de Entrega: 28 de Mayo de 2023\n"
   ],
   "metadata": {
    "id": "ZIsH_HPjvhdS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Sobre el Documento"
   ],
   "metadata": {
    "id": "2Ua7LZykxSuR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Indice:\n",
    ">   1.  ...................................................... (Parte 1.) Descripcion Teorica\n",
    ">   2.  ...................................................... (Parte 2.) \"Numerico\" [(a) y (b)]\n",
    ">   3.  ...................................................... Palabras Finales\n",
    ">\n",
    "> ### Componentes:\n",
    "> El presente documento es el solicitado en formato \"*jupyter Notebook*\" de la tarea numerica solicitada. En esta se encontraran cuadros de codigo, algunas anotaciones, cuadros de texto explicativos además de algunos ejemplos si la situacion lo amerita. Esto con el fin de servir como argumentacion o solucion directa a la evaluacion.\n",
    ">\n",
    "> Los cuadros de texto corresponeran a la información sobre el codigo que le prosigue siguiendo un formato de \"*texto arriba, codigo abajo*\", el cual intentara cumplir el objetivo de explicar el funcionamiento del codigo en el presente. Pese a esto también existiran anotaciones extras en formato \"# *Anotaciones*\" color \"verde\" en el/los codigos que deberia tener un formato similar al siguiente:\n",
    "```\n",
    "1 # Anotacion\n",
    "2 Este es un codigo:              # Anotacion\n",
    "3     sirve para ejemplificar     # Anotacion\n",
    "4     el formato que se dijo      # Anotacion \n",
    "5                                 #\n",
    "6 previamente                     # Anotacion\n",
    "```\n",
    "> Estos seran parte del codigo pero no tendran funcion ninguna más que ser anotaciones insertadas por los desarrolladores para uso personal, aunque de ser necesario puede utilizarse como información sobre el documento/codigo.\n",
    "> ### Orden:\n",
    "> El documento estara estructurado mediante diferentes partes que se enumeran en el \"Indice\", en la primera parte (Parte 1.) y la segunda (Parte 2.) se encuentran las soluciones respectivas, mientra que en \"Palabras Finales\", comentarios respectivos a la realizacion del documeno o tarea.\n",
    ">\n",
    "> Es importante informar que de ser necesario, se utilizaran codigos compartidos con el fin de mantener el orden y optimizar el proceso, el cual será denotado por una anotacion al inicio del cuadro de codigo de forma \"codigo compartido\". Para poder utilizar el codigo donde esté siendo reutilizado, será necesario inicializar el original. Un caso será la siguiente línea de codigos que permitira el acceso a librerias que podrian ser utilizadas.\n"
   ],
   "metadata": {
    "id": "9GiLnnngJaEl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Parte 1. Descripcion Teorica"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tNuta-PFSkTC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Dado a las caracteristicas de la entrega, en la cual ademas del documento prescente, se entrego un informe en formato PDF donde se contendran las soluciones respectivas."
   ],
   "metadata": {
    "collapsed": false,
    "id": "rX0i-LEuSkTD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Parte 2. \"Numerico\""
   ],
   "metadata": {
    "collapsed": false,
    "id": "i3Xdqgi7SkTD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> La parte 2 de la tarea solicitada, se baso en dos diferentes estudios (aunque relacionados en la tematica presente) a programar. Esto se dividion en dos etapas para realizar la respectiva funcion.\n",
    "> Sin embargo, estas dos etapas llegaron a tener documentos o funcionalidades en comun, para lo cual se agregaron los siguientes codigos y celdas de codigos para optimizar el orden y proceso con estas funciones."
   ],
   "metadata": {
    "collapsed": false,
    "id": "nZZRfcF3SkTE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================================    ______________________\n",
    "########################## Librerias Importadas ################################    | Anotaciones Extras |\n",
    "# Codigo compartido |--> Librerias export importadas                                |____________________|\n",
    "import numpy as np\n",
    "########################## Librerias Importadas ################################\n",
    "# =============================================================================="
   ],
   "metadata": {
    "id": "ud8Ehlcbzv3D",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.849257100Z",
     "start_time": "2023-05-28T06:12:53.818340300Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# ==============================================================================    ______________________\n",
    "########################### Valores Utilizados# ################################    | Anotaciones Extras |\n",
    "# Lista de tuplas o instancias.                                                     |____________________|\n",
    "D = [(9.0,7.0,0.0), (2.0,5.0,1.0), (3.2,4.94,1.0), (9.1,7.46,0.0),\n",
    "     (1.6,4.83,1.0), (8.4,7.46,0.0), (8.0,7.28,0.0), (3.1,4.58,1.0),\n",
    "     (6.3,9.14,0.0), (3.4,5.36,1.0)]\n",
    "\n",
    "\n",
    "# Vector o punto de realizacion\n",
    "x0 = (8, 7)                                                                      # Tupla que representa al vector\n",
    "\n",
    "\n",
    "# Condiciones iniciales/constantes\n",
    "k = 1000                                                                         # Cantidad de iteraciones\n",
    "l = 0.01                                                                         # Learning rate de la red neuronal\n",
    "########################### Valores Utilizados# ################################\n",
    "# =============================================================================="
   ],
   "metadata": {
    "id": "kp3lRUeASkTG",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.876186Z",
     "start_time": "2023-05-28T06:12:53.839283700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### (a) Programar algoritmo que \"Entrenara\" redes neuronales\n",
    "> Durante la etapa \"$a)$\", se solicito la realizacion del algoritmo llamado \"gradiente conjugado\" para los diferentes \"$k$\" dado a $k = 1,...,M$.\n",
    "> El algoritmo tiene la principal funcion de realizar la siguiente iteracion:\n",
    ">\\begin{align*} w_{1}^{k+1} &= w_{1}^{k}-l\\frac{\\partial C}{\\partial w_{1}} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\\\ w_{2}^{k+1} &= w_{2}^{k}-l\\frac{\\partial C}{\\partial w_{2}} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\\\ b^{k+1} &= b^{k}-l\\frac{\\partial C}{\\partial b} (w_{1}^{k}, w_{2}^{k}, b^{k}) \\end{align*}\n",
    "> Con $(w_{1}^{0}, w_{2}^{0}, b^{0}) \\in \\mathbb{R}^{3}$ como inicio, dados y aleatorios.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZavpqEIJSkTG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Considerando el algoritmo propuesto, el codigo (o los respectivos cuadros de codigos) fueron basados en\n",
    "> torno a las diferentes funciones o calculos realizados en la parte teorica. Especificamente para el caso\n",
    "> de las derivadas parciales y las realizaciones. De esta manera, se llego a tres principales grupos de\n",
    "> funciones que estaban prescentes durante el proceso: \\\n",
    ">\n",
    "> \\begin{enumerate} 1. & \\text{Realizacion} \\\\ 2. & \\text{Derivadas costo} \\\\ 3. & \\text{Gradiente Conjugado} \\end{enumerate} \\\n",
    "> Estos grupos fueron ordenados en respectivos cuadros de codigos de la manera siguiente:\n",
    "> #### 1. Realizacion Red Neuronal \\\n",
    "> En este caso, se encontrarian las funciones que realizan el proceso de obtencion de una red neuronal\n",
    "> de manera estandar siempre y cuando cumpla las mismas condiciones a las que fueron impuestas inicialmente. \\\n",
    "> De las funciones realizadas, se tienen las siguientes: \\\n",
    ">\n",
    ">  $\\rightarrow$ \"sigmoid\": Tiene la importancia de ser la funcion de activacion sigma ($\\sigma$).\n",
    ">  Teoricamente es de la forma:\n",
    "> \\begin{align*} \\sigma(s) &= \\frac{e^{s}}{(1+e^{s})} \\\\ \\sigma'(s) &= \\frac{e^{s}}{(1+e^{s})^{2}} \\end{align*}\n",
    "> Donde $\\sigma(s)$ corresponde a la funcion en especificio y, $\\sigma'(s)$ a su derivada respectivamente. \\\n",
    ">\n",
    ">  $\\rightarrow$ \"f\": Analogo a los resultados obtenidos en el ambito teorico, corresponde a los puntos\n",
    "> evaluador por la funcion de activacion, asi permitiendo reescribir la realizacion $\\mathcal{R}(\\Phi)$.\n",
    "> Teoricamente serian de la fomrma: \\\n",
    "> \\begin{align*} f(x) &= w_{1}x_{0,1}+w_{2}x_{0,2}+b \\\\ \\implies \\mathcal{R}(\\Phi)(x_{0}) &= \\sigma(w_{1}x_{0,1}+w_{2}x_{0,2}+b) \\\\ \\iff \\mathcal{R}(\\Phi)(x_{0}) &= \\sigma(f(x_{0})) & (\\sigma o f)(x_{0})\\  (\\text{analogo})\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================    ______________________   \n",
    "############################## Realizacion Red #################################    | Anotaciones Extras |\n",
    "# Funcion sigmoid                                                                   |____________________|    \n",
    "# Toma dos numeros y devuelve la funcion sigmoid o su derivada\n",
    "#           => 0, devuelve la funcion\n",
    "#           => 1, devuelve la derivada en torno a s\n",
    "def sigmoid(s,tipo):\n",
    "    e = np.exp(s)                                                                # Evalua la exponencial en el punto \"s\"\n",
    "    denominador = (1+e)                                                          # Denominador de la funcion\n",
    "    if tipo == 0:                                                                # Condicional para evaluar funcion\n",
    "        sigmoide = (e)/(denominador)\n",
    "        return sigmoide\n",
    "    elif tipo == 1:                                                              # Condicional para evaluar derivada\n",
    "        sigmoide = (e)/((denominador)**2)\n",
    "        return sigmoide\n",
    "\n",
    "\n",
    "# Funcion f(X)\n",
    "# Toma una tupla de vectores (x1,x2) y una tupla phi (w1, w2, b) y,\n",
    "# devuelve la realizacion no evaluada de la red neuronal \n",
    "def f(phi,x):\n",
    "    x1,x2 = x                                                                    # Obtiene valores Tupla\n",
    "    w1,w2,b = phi                                                                # Obtiene valores Phi\n",
    "    fx = w1*x1+w2*x2+b                                                           # Calcula pre-realizacion\n",
    "    return fx\n",
    "\n",
    "# Funcion Realizacion de la red neuronal (Rphi(X0))\n",
    "# Toma la tupla de valores y el vector Phi para devolver\n",
    "# la realizacion de la red neuronal.\n",
    "# Teoricamente corresponde al calculo de:\n",
    "# R(Phi)(X0) = sigma(f(x))\n",
    "def realizacion(phi,x,dif=0): \n",
    "    fx = f(phi,x)                                                                # obtencion f(x)\n",
    "    gof = sigmoid(fx,dif)                                                        # composicion\n",
    "    return gof\n",
    "############################## Realizacion Red #################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "Jy1pGjynSkTH",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.876186Z",
     "start_time": "2023-05-28T06:12:53.856238500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================    ______________________\n",
    "############################ Derivadas Costo ###################################    | Anotaciones Extras |  \n",
    "# Funcion derivada parcial                                                          |____________________|  \n",
    "# Funcion que realiza el calculo de la derivada parcial en torno a los valores      \n",
    "# del peso (w1, w2) y el bias (b) de un phi dado. Esto para una coleccion de\n",
    "# datos input-output,  devolviendo un vector de tuplas de la derivada parcial.\n",
    "def derivadaParcial(phi,datos):\n",
    "    gradD = []                                                                   # Caso base, lista vacia\n",
    "    for i in range(len(datos)): \n",
    "        x1d,x2d,xfd = datos[i]                                                   # Obtencion de tuplas para cada dato de la coleccion\n",
    "        dato = x1d,x2d                                                           # Definir una tupla para input\n",
    "        fx = f(phi,dato)                                                         # Obtencion de pre-realizacion      \n",
    "        C = sigmoid(fx,0)-xfd                                                    # -|--->|Definir una base para derivadas parciales\n",
    "        dC = sigmoid(fx,1)                                                       # -|    |por indice de la forma: (R(phi)(x0)-xf)(R(phi)'(x0))  \n",
    "        dw1 = C*dC*x1d                                                           # (R(phi)(x0)-xf)(R(phi)'(x0))x01\n",
    "        dw2 = C*dC*x2d                                                           # (R(phi)(x0)-xf)(R(phi)'(x0))x02\n",
    "        db = C*dC                                                                # (R(phi)(x0)-xf)(R(phi)'(x0))\n",
    "        gradx = dw1,dw2,db                                                       # -|          \n",
    "        gradd = gradD+[gradx]                                                    #  |--->|Definir recursion para tupla planteada         \n",
    "        gradD = gradd                                                            # -|    |el nombre gradd se le da en honor al gradiente \n",
    "        return gradD                                                               \n",
    "\n",
    "\n",
    "# Funcion derivada de costos\n",
    "# Basandose en que la derivada de las sumas es la suma de derivadas (algebra\n",
    "# de derivadas), suma iterando esto para cada dato ingresado dado a las \n",
    "# derivadas parciales previamente calculadas, devolviendo una lista que \n",
    "#contiene a las derivadas parciales del costo.\n",
    "def dcosto(phi,datas):                                                        \n",
    "    datos = derivadaParcial(phi,datas)                                           # Se obtienen la coleccion de tuplas de derivadas parciales\n",
    "    dw1 = []                                                                     # -|                                          \n",
    "    dw2 = []                                                                     #  |--->|Caso base para las derivadas parciales\n",
    "    db = []                                                                      # -|                                         \n",
    "    for i in range(len(datos)):                                                        \n",
    "        xc1, xc2, xcb = datos[i]                                                 # Se obtienen los datos de las tuplas       \n",
    "        w1 = dw1+[xc1]                                                           # -|\n",
    "        dw1 = w1                                                                 # -|--->|Crear coleccion solo de derivadas en torno a w1\n",
    "        w2 = dw2+[xc2]                                                           # -|    \n",
    "        dw2 = w2                                                                 # -|--->|Crear coleccion solo de derivadas en torno a w2\n",
    "        b = db+[xcb]                                                             # -| \n",
    "        db = b                                                                   # -|--->|Crear coleccion solo de derivadas en torno a b\n",
    "        return [sum(dw1),sum(dw2),sum(db)]                                       # Suma de coleccion de derivadas y se crea coleccion de estas     \n",
    "############################ Derivadas Costo ###################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "kJ33dmDbSkTH",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.885161500Z",
     "start_time": "2023-05-28T06:12:53.870201400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# ============================= Tarea 2, Parte 2.a =============================    ______________________\n",
    "########################## Gradiente Conjugado #################################    | Anotaciones Extras |\n",
    "# Funcion gradiente Conjugado                                                       |____________________|\n",
    "# Toma una cantidad \"k\" de iteraciones, una tasa \"l\" de aprendizaje y una \n",
    "# coleccion de datos para obtener una aproximacion de la realizacion de una \n",
    "# red neuronal desconocida. Esto en base a un phi aleatorio otorgado.\n",
    "def gradienteConjugado(k,l,datos):  \n",
    "    w1 = np.random.uniform(-1,1)                                                 # -|                  \n",
    "    w2 = np.random.uniform(-1,1)                                                 #  |--->|Definicion de valores w1, w2, b aleatorios entre [-1,1] \n",
    "    b = np.random.uniform(-1,1)                                                  # -|                                       \n",
    "    def gradienteNC(w1,w2,b,k,l,datos):                                          # Se define una funcion que realizara los calculos\n",
    "        phi = w1,w2,b                                                            # Definir una realizacion (Phi) base\n",
    "        if k == 0:                                                               # Caso base en recursividad.\n",
    "            return w1,w2,b                                                     \n",
    "        else:                                                                   \n",
    "            w11 = w1-(l)*float((dcosto(phi,datos)[0]))                           # -|    |Iteracion dada que el gradiente debe cumplir.\n",
    "            w22 = w2-(l)*float((dcosto(phi,datos)[1]))                           #  |--->|Esta obtiene el caso para k+1 iteraciones dado k.\n",
    "            bb = b-(l)*float((dcosto(phi,datos)[2]))                             # -|    |Se realiza para las componentes w1, w2 y b del Phi.\n",
    "            w1 = w11                                                           \n",
    "            w2 = w22                                                            \n",
    "            b = bb                                                            \n",
    "            return gradienteNC(w1,w2,b,k-1,l,datos)                              # Realiza iteracion.\n",
    "    Cgradiente = gradienteNC(w1,w2,b,k,l,datos)                                  # Se llama a la funcion para realizar la operacion.\n",
    "    return Cgradiente\n",
    "########################## Gradiente Conjugado #################################\n",
    "# ============================= Tarea 2, Parte 2.a ============================="
   ],
   "metadata": {
    "id": "xXQ4wepISkTI",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.896131400Z",
     "start_time": "2023-05-28T06:12:53.888153600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### (b) Realizar M = 1000 iteraciones del algoritmo con el fin de entrentar la red $\\Phi$. Ademas, encontrar el valor de la realizacion en el punto $(8, 7)$\n",
    "> Dado $D$ con 10 instancias consistentes de dos valores input y un tercero de output en {$0,1$} de la siguiente manera: \\\n",
    "> $D=${{$9, 7.0, 0$}, {$2, 5.0, 1$}, {$3.2, 4.94, 1$}, {$9.1, 7.46, 0$}, {$1.6, 4.83, 1$}, {$8.4, 7.46, 0$}, {$8, 7.28, 0$}, {$3.1, 4.58, 1$}, {$6.3, 9.14, 0$}, {$3.4, 5.36, 1$}} \\\n",
    "> Realizar M=1000 iteraciones del gradiente conjugado (anterior) para lograr el objetivo.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "M5opf7leSkTI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS\n",
      "\n",
      "\n",
      "\n",
      "Para laiteracion numero 1000 ,el vector Phi deberia estar dado por: (-0.13971261355054537, -0.46446144602402906, -0.2859176482725994)\n",
      "A su vez, la realizacion estaria dada por: 0.009425664482355767\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================= Tarea 2, Parte 2.b =============================    ______________________\n",
    "########################## Gradiente Conjugado #################################    | Anotaciones Extras |\n",
    "# Desarrollo final                                                                  |____________________|\n",
    "#   I. Ejecutar el gradiente para obtener el Phi = (wk1, wk2, bk2)\n",
    "#   II. Llamando \"phi = gradiente conjugado\", ejercutamos la\n",
    "#       realizacion en (8, 7) y phi\n",
    "#   III. Printeamos n resultados con la cantidad \"n\", numero natural a gusto\n",
    "print('RESULTADOS')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "# I.\n",
    "# Aplicacion del gradiente conjugado\n",
    "phiconjugado = gradienteConjugado(k,l,D)\n",
    "# Mostrar resultados\n",
    "print('Para laiteracion numero', k,',el vector Phi deberia estar dado por:'\n",
    ",phiconjugado)\n",
    "# II.\n",
    "# Obtencion de red neuronal\n",
    "redrealizada = realizacion(phiconjugado,x0,0)\n",
    "# Mostrar resultados\n",
    "print('A su vez, la realizacion estaria dada por:',redrealizada)\n",
    "print()\n",
    "print()\n",
    "# III.\n",
    "#n = 25\n",
    "#print('En', n,'Tuplas (phi) aleatorias, se tiene el siguiente comportamiento:')\n",
    "#print()\n",
    "#for i in range(n):\n",
    "#    grad = gradienteConjugado(k,l,D)\n",
    "#    print(i+1,'| Gradiente Conjugado (phi):', grad)\n",
    "#    real = realizacion(grad,x0,0)\n",
    "#    print('     Realizacion:', real)\n",
    "#    print()\n",
    "########################## Gradiente Conjugado #################################\n",
    "# ============================= Tarea 2, Parte 2.b ============================="
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFcg18SzSkTI",
    "outputId": "08274656-62e8-4ecd-a1f1-a59d66a7a899",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:53.960958700Z",
     "start_time": "2023-05-28T06:12:53.899123700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## Palabras Finales:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "CbTNIbOwSkTJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> $\\int_{E.Media}^{Dormir} Beauchef dx = felicidad $"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8ohZdTxuSkTK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print('Hello, world!')"
   ],
   "metadata": {
    "id": "Zm9EsKm2SkTK",
    "ExecuteTime": {
     "end_time": "2023-05-28T06:12:54.003843400Z",
     "start_time": "2023-05-28T06:12:53.961956600Z"
    }
   }
  }
 ]
}
